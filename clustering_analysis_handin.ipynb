{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02807 - Computational Tools for Data Science\n",
    "This notebook is intended to showcase the methods used in the project, for the scripts used to produce the results in the report plaese refer to run_clustering_methods_and_classifiers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-06T19:31:40.473497Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from utils.scoring import purity_score, score_clustering, clustering_classification_report\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "from utils.plotting import plot_2d_clusters\n",
    "from xgboost import XGBClassifier\n",
    "from spectralnet import SpectralNet\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from utils.scoring import clustering_classification_report, score_clustering\n",
    "import torch\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Genral Utils for Clustering                                      #\n",
    "####################################################################\n",
    "\n",
    "def format_clustering_metrics(train_scores, test_scores, **kwargs):\n",
    "    results = kwargs\n",
    "    results.update({\n",
    "        'Train_DB': train_scores[0],\n",
    "        'Train_Sil': train_scores[1],\n",
    "        'Train_Pur': train_scores[2],\n",
    "        'Test_DB': test_scores[0],\n",
    "        'Test_Sil': test_scores[1],\n",
    "        'Test_Pur': test_scores[2],\n",
    "    })\n",
    "    return results\n",
    "\n",
    "def format_classification_metrics(train_metrics, test_metrics, **kwargs):\n",
    "    results = kwargs\n",
    "    results.update({\n",
    "        'Train_Acc': train_metrics['accuracy'],\n",
    "        'Train_F1': train_metrics['f1'],\n",
    "        'Train_Recall': train_metrics['recall'],\n",
    "        'Train_Precision': train_metrics['precision'],\n",
    "        'Test_Acc': test_metrics['accuracy'],\n",
    "        'Test_F1': test_metrics['f1'],\n",
    "        'Test_Recall': test_metrics['recall'],\n",
    "        'Test_Precision': test_metrics['precision'],\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:31:34.936935900Z",
     "start_time": "2024-12-03T16:57:28.823250Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "X = data.drop(['Class', 'Amount', 'Time'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "X_scale = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# sample data to reduce class imbalance\n",
    "non_fraud_df = X_scale[y == 0][:2000]\n",
    "fraud_df = X_scale[y == 1]\n",
    "\n",
    "X_sample = np.vstack([non_fraud_df, fraud_df])\n",
    "fraud_idx = np.zeros(len(X_sample))\n",
    "fraud_idx[-len(fraud_df):] = 1\n",
    "\n",
    "indices = np.arange(len(X_sample))\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X_sample, fraud_idx, indices, test_size=0.2, random_state=42, stratify=fraud_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(X_train, X_test, y_train, y_test):\n",
    "    param_dist = {\n",
    "        'C': np.logspace(-2, 2, 20),\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "    # Use RandomizedSearchCV for faster hyperparameter tuning\n",
    "    rand_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring='neg_log_loss',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = rand_search.best_estimator_\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "\n",
    "    # Get classification report as a dictionary\n",
    "    classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Add classification report and best hyperparameters to metrics\n",
    "    metrics['classification_report'] = classification_report_dict\n",
    "    metrics['best_hyperparameters'] = rand_search.best_params_\n",
    "\n",
    "    # Convert metrics dictionary to a DataFrame\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kmeans(X_train, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    return kmeans\n",
    "\n",
    "def extract_kmeans_cluster_labels(kmeans, X):\n",
    "    return kmeans.predict(X)\n",
    "\n",
    "def evaluate_kmeans(X_train, X_test, y_train, y_test, n_clusters):\n",
    "    kmeans = fit_kmeans(X_train, n_clusters)\n",
    "    train_cluster_labels = extract_kmeans_cluster_labels(kmeans, X_train)\n",
    "    test_cluster_labels = extract_kmeans_cluster_labels(kmeans, X_test)\n",
    "\n",
    "    train_clustering_scores = score_clustering(X_train, y_train, train_cluster_labels)\n",
    "    test_clustering_scores = score_clustering(X_test, y_test, test_cluster_labels)\n",
    "\n",
    "    train_classification_report = clustering_classification_report(train_cluster_labels, y_train)\n",
    "    test_classification_report = clustering_classification_report(test_cluster_labels, y_test)\n",
    "    clustering_metrics = format_clustering_metrics(train_clustering_scores, test_clustering_scores, n_clusters=n_clusters)\n",
    "    classification_metrics = format_classification_metrics(train_classification_report, test_classification_report, n_clusters=n_clusters)\n",
    "    return clustering_metrics, classification_metrics\n",
    "\n",
    "def score_kmeans(X_train, X_test, y_train, y_test, cluster_counts=None):\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = [2, 3, 5, 10]\n",
    "    clustering_metrics_list = []\n",
    "    classification_metrics_list = []\n",
    "    for n_clusters in cluster_counts:\n",
    "        clustering_scores, classification_scores = evaluate_kmeans(X_train, X_test, y_train, y_test, n_clusters)\n",
    "        clustering_metrics_list.append(clustering_scores)\n",
    "        classification_metrics_list.append(classification_scores)\n",
    "    return clustering_metrics_list, classification_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:31:34.938123200Z",
     "start_time": "2024-12-03T16:57:29.632083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>Train_DB</th>\n",
       "      <th>Train_Sil</th>\n",
       "      <th>Train_Pur</th>\n",
       "      <th>Test_DB</th>\n",
       "      <th>Test_Sil</th>\n",
       "      <th>Test_Pur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_clusters  Train_DB  Train_Sil  Train_Pur  Test_DB  Test_Sil  Test_Pur\n",
       "0           2      0.84       0.63       0.56     0.79      0.64      0.60\n",
       "1           3      1.09       0.48       0.79     1.05      0.47      0.81\n",
       "2           5      1.97       0.12       0.77     1.93      0.13      0.79\n",
       "3          10      1.96       0.11       0.82     1.74      0.12      0.83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = [2, 3, 5, 10]\n",
    "cluster_metrics, classification_metrics = score_kmeans(X_train, X_test, y_train, y_test)\n",
    "results_df = pd.DataFrame(cluster_metrics)\n",
    "reports_df = pd.DataFrame(classification_metrics)\n",
    "\n",
    "results_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dbscan(X_train, eps):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    dbscan.fit(X_train)\n",
    "    return dbscan\n",
    "\n",
    "def test_dbscan(dbscan, X_train, X_test):\n",
    "    core_samples_mask = dbscan.core_sample_indices_\n",
    "    core_points = X_train[core_samples_mask]\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=1).fit(core_points)\n",
    "    distances, indices = nn.kneighbors(X_test)\n",
    "\n",
    "    test_clusters = np.array([dbscan.labels_[core_samples_mask[i]] if distances[j] < dbscan.eps else -1\n",
    "                            for j, i in enumerate(indices.flatten())])\n",
    "    return test_clusters\n",
    "\n",
    "def evaluate_dbscan(X_train, X_test, y_train, y_test, eps, min_samples):\n",
    "    dbscan = fit_dbscan(X_train, eps, min_samples)\n",
    "    train_clusters = dbscan.labels_\n",
    "    test_clusters = test_dbscan(dbscan, X_train, X_test)\n",
    "\n",
    "    train_clustering_scores = score_clustering(X_train, y_train, train_clusters)\n",
    "    test_clustering_scores = score_clustering(X_test, y_test, test_clusters)\n",
    "\n",
    "    train_classification_report = clustering_classification_report(train_clusters, y_train)\n",
    "    test_classification_report = clustering_classification_report(test_clusters, y_test)\n",
    "\n",
    "    clustering_metrics = format_clustering_metrics(train_clustering_scores, test_clustering_scores, eps=eps, min_samples=min_samples)\n",
    "    classification_metrics = format_classification_metrics(train_classification_report, test_classification_report, eps=eps, min_samples=min_samples)\n",
    "    return clustering_metrics, classification_metrics\n",
    "\n",
    "def score_dbscan(X_train, X_test, y_train, y_test, eps=None, min_samples=None):\n",
    "    if eps is None:\n",
    "        eps = [0.2, 0.3, 0.4]\n",
    "    if min_samples is None:\n",
    "        min_samples = [5, 10]\n",
    "    clustering_metrics_list = []\n",
    "    classification_metrics_list = []\n",
    "    for e in eps:\n",
    "        for ms in min_samples:\n",
    "            try:\n",
    "                clustering_scores, classification_scores = evaluate_dbscan(X_train, X_test, y_train, y_test, e, ms)\n",
    "                clustering_metrics_list.append(clustering_scores)\n",
    "                classification_metrics_list.append(classification_scores)\n",
    "            except ValueError:\n",
    "                print(f'Fitting DBSCAN with eps={e} and min_samples={ms} leads to a single cluster. Skipping...')\n",
    "    return clustering_metrics_list, classification_metrics_list\n",
    "\n",
    "def fit_dbscan(X_train, eps, min_samples):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(X_train)\n",
    "    return dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:57:30.425125Z",
     "start_time": "2024-12-03T16:57:30.002282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eps</th>\n",
       "      <th>min_samples</th>\n",
       "      <th>Train_DB</th>\n",
       "      <th>Train_Sil</th>\n",
       "      <th>Train_Pur</th>\n",
       "      <th>Test_DB</th>\n",
       "      <th>Test_Sil</th>\n",
       "      <th>Test_Pur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>10</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>15</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>5</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>10</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.20</td>\n",
       "      <td>10</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.20</td>\n",
       "      <td>15</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eps  min_samples  Train_DB  Train_Sil  Train_Pur  Test_DB  Test_Sil  \\\n",
       "0   0.10            5      1.60      -0.22       0.49     1.57     -0.27   \n",
       "1   0.10           10      2.19      -0.19       0.38     2.11     -0.22   \n",
       "2   0.10           15      2.28      -0.23       0.31     2.31     -0.26   \n",
       "3   0.15            5      1.54       0.22       0.83     1.84      0.15   \n",
       "4   0.15           10      1.26       0.10       0.82     1.46      0.08   \n",
       "5   0.15           15      1.78       0.35       0.80     1.78      0.34   \n",
       "6   0.20            5      1.87       0.31       0.83     1.93      0.25   \n",
       "7   0.20           10      1.76       0.28       0.84     1.97      0.25   \n",
       "8   0.20           15      1.65       0.28       0.84     1.60      0.24   \n",
       "9   0.25            5      1.26       0.54       0.31     1.56      0.56   \n",
       "10  0.25           10      1.47       0.49       0.39     1.27      0.48   \n",
       "11  0.25           15      2.19       0.51       0.53     2.40      0.48   \n",
       "\n",
       "    Test_Pur  \n",
       "0       0.42  \n",
       "1       0.35  \n",
       "2       0.28  \n",
       "3       0.79  \n",
       "4       0.78  \n",
       "5       0.75  \n",
       "6       0.82  \n",
       "7       0.82  \n",
       "8       0.82  \n",
       "9       0.37  \n",
       "10      0.47  \n",
       "11      0.58  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = [0.1, 0.15, 0.2, 0.25]\n",
    "min_samples = [5, 10, 15]\n",
    "dbscan_cluster_metrics, dbscan_classification_metrics = score_dbscan(X_train, X_test, y_train, y_test, eps, min_samples)\n",
    "dbscan_results_df = pd.DataFrame(dbscan_cluster_metrics)\n",
    "dbscan_reports_df = pd.DataFrame(dbscan_classification_metrics)\n",
    "\n",
    "dbscan_results_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_manual_sparse(X, gamma=1.0, n_neighbors=1000):\n",
    "    \"\"\"\n",
    "    Compute a sparse RBF kernel using nearest neighbors.\n",
    "    \"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    # Compute sparse RBF kernel\n",
    "    row_indices = np.repeat(np.arange(X.shape[0]), n_neighbors)\n",
    "    col_indices = indices.flatten()\n",
    "    exp_values = np.exp(-gamma * distances.flatten()**2)\n",
    "\n",
    "    affinity_matrix = csr_matrix((exp_values, (row_indices, col_indices)), shape=(X.shape[0], X.shape[0]))\n",
    "    return affinity_matrix\n",
    "\n",
    "\n",
    "def construct_laplacian(affinity_matrix):\n",
    "    \"\"\"\n",
    "    Construct the normalized graph Laplacian.\n",
    "    Supports both dense and sparse affinity matrices.\n",
    "    \"\"\"\n",
    "    # Ensure affinity_matrix is sparse\n",
    "    if not isinstance(affinity_matrix, csr_matrix):\n",
    "        affinity_matrix = csr_matrix(affinity_matrix)\n",
    "\n",
    "    # Compute degree values\n",
    "    degree_values = np.array(affinity_matrix.sum(axis=1)).flatten()\n",
    "    degree_values[degree_values == 0] = 1e-10  # Avoid division by zero\n",
    "\n",
    "    # Create D^(-1/2) as a sparse diagonal matrix\n",
    "    d_inv_sqrt = 1.0 / np.sqrt(degree_values)\n",
    "    d_inv_sqrt_sparse = diags(d_inv_sqrt)\n",
    "\n",
    "    # Compute the normalized Laplacian\n",
    "    laplacian = diags([1.0], [0], shape=affinity_matrix.shape) - d_inv_sqrt_sparse @ affinity_matrix @ d_inv_sqrt_sparse\n",
    "\n",
    "    return laplacian\n",
    "\n",
    "\n",
    "def kmeans_pp_init(X, n_clusters, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = []\n",
    "\n",
    "    # Step 1: Randomly select the first centroid\n",
    "    first_centroid_idx = np.random.randint(0, n_samples)\n",
    "    centroids.append(X[first_centroid_idx])\n",
    "\n",
    "    # Step 2: Select remaining centroids\n",
    "    for _ in range(1, n_clusters):\n",
    "        # Compute distances from each point to the nearest centroid\n",
    "        sq_norms_X = np.sum(X ** 2, axis=1, keepdims=True)  # Shape: (n_samples, 1)\n",
    "        sq_norms_centroids = np.sum(np.array(centroids) ** 2, axis=1, keepdims=True).T  # Shape: (1, len(centroids))\n",
    "        distances = sq_norms_X + sq_norms_centroids - 2 * np.dot(X, np.array(centroids).T)  # Shape: (n_samples, len(centroids))\n",
    "        distances = np.sqrt(np.maximum(distances, 0))  # Ensure non-negativity\n",
    "\n",
    "        # For each point, find the minimum distance to any centroid\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "\n",
    "        # Compute the probability distribution for the next centroid\n",
    "        probabilities = min_distances ** 2 / np.sum(min_distances ** 2)\n",
    "\n",
    "        # Randomly select the next centroid based on the probabilities\n",
    "        next_centroid_idx = np.random.choice(n_samples, p=probabilities)\n",
    "        centroids.append(X[next_centroid_idx])\n",
    "\n",
    "    return np.array(centroids)\n",
    "\n",
    "\n",
    "def kmeans_manual(X, n_clusters, max_iter=500, tol=1e-5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    centroids = kmeans_pp_init(X, n_clusters, seed)\n",
    "    for _ in range(max_iter):\n",
    "        sq_norms_X = np.sum(X ** 2, axis=1, keepdims=True)\n",
    "        sq_norms_centroids = np.sum(centroids ** 2, axis=1, keepdims=True).T\n",
    "        distances = sq_norms_X + sq_norms_centroids - 2 * np.dot(X, centroids.T)\n",
    "        distances = np.sqrt(np.maximum(distances, 0))\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([\n",
    "            X[labels == i].mean(axis=0) if np.any(labels == i) else X[np.random.randint(0, X.shape[0])]\n",
    "            for i in range(n_clusters)\n",
    "        ])\n",
    "        if np.allclose(centroids, new_centroids, atol=tol):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return labels, centroids\n",
    "\n",
    "def derive_cluster_mapping(y_train, train_cluster_labels):\n",
    "    \"\"\"\n",
    "    Derives a mapping of cluster labels to actual class labels using the training set.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for cluster in np.unique(train_cluster_labels):\n",
    "        mask = (train_cluster_labels == cluster)\n",
    "        if np.any(mask):  # Ensure the mask selects at least one element\n",
    "            mode_result = mode(y_train[mask], nan_policy='omit')  # Handle potential NaNs\n",
    "            cluster_mode = mode_result.mode\n",
    "            if isinstance(cluster_mode, np.ndarray) and cluster_mode.size > 0:\n",
    "                mapping[cluster] = cluster_mode[0]  # Access the mode value if it's an array\n",
    "            else:\n",
    "                mapping[cluster] = cluster_mode  # Directly assign the scalar value\n",
    "        else:\n",
    "            raise ValueError(f\"Cluster {cluster} is empty. Check clustering assignments.\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def map_clusters(cluster_labels, mapping):\n",
    "    \"\"\"\n",
    "    Maps cluster labels to class labels based on the derived mapping.\n",
    "    \"\"\"\n",
    "    return np.array([mapping[label] for label in cluster_labels])\n",
    "\n",
    "\n",
    "def spectral_clustering(X, n_clusters, gamma=1.0, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    affinity_matrix = rbf_kernel_manual_sparse(X, gamma=gamma)\n",
    "    laplacian = construct_laplacian(affinity_matrix)\n",
    "    eigvals, eigvecs = eigsh(laplacian, k=n_clusters, which='SM')  # 'SM' = Smallest Magnitude\n",
    "    # Select eigenvectors corresponding to the smallest n_clusters eigenvalues\n",
    "    eigvecs_subset = eigvecs[:, np.argsort(eigvals)[:n_clusters]]\n",
    "    normalized_eigvecs = eigvecs_subset / np.linalg.norm(eigvecs_subset, axis=1, keepdims=True)\n",
    "    labels, _ = kmeans_manual(normalized_eigvecs, n_clusters=n_clusters, seed=seed)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def evaluate_spectral(X_train, X_test, y_train, y_test, n_clusters, gamma=1.0):\n",
    "    # Step 1: Perform spectral clustering\n",
    "    train_cluster_labels = spectral_clustering(X_train, n_clusters, gamma)\n",
    "\n",
    "    # Step 2: Derive cluster-to-class mapping from training data\n",
    "    cluster_to_class_mapping = derive_cluster_mapping(y_train, train_cluster_labels)\n",
    "\n",
    "    # Step 3: Assign test data to nearest training clusters\n",
    "    train_centroids = np.array([X_train[train_cluster_labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "    # Compute pairwise distances between test data and training centroids\n",
    "    sq_norms_X_test = np.sum(X_test**2, axis=1, keepdims=True)  # Shape: (n_test_samples, 1)\n",
    "    sq_norms_train_centroids = np.sum(train_centroids**2, axis=1, keepdims=True).T  # Shape: (1, n_clusters)\n",
    "    distances = sq_norms_X_test + sq_norms_train_centroids - 2 * np.dot(X_test, train_centroids.T)  # Shape: (n_test_samples, n_clusters)\n",
    "    distances = np.sqrt(np.maximum(distances, 0))  # Ensure non-negativity\n",
    "\n",
    "    test_cluster_labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Step 4: Map cluster labels to class labels\n",
    "    train_aligned_labels = map_clusters(train_cluster_labels, cluster_to_class_mapping)\n",
    "    test_aligned_labels = map_clusters(test_cluster_labels, cluster_to_class_mapping)\n",
    "\n",
    "    # Step 5: Compute metrics (you can plug in your scoring and classification report functions here)\n",
    "    train_clustering_scores = score_clustering(X_train, y_train, train_aligned_labels)\n",
    "    test_clustering_scores = score_clustering(X_test, y_test, test_aligned_labels)\n",
    "\n",
    "    train_classification_report = clustering_classification_report(train_aligned_labels, y_train)\n",
    "    test_classification_report = clustering_classification_report(test_aligned_labels, y_test)\n",
    "\n",
    "    clustering_metrics = format_clustering_metrics(train_clustering_scores, test_clustering_scores, n_clusters=n_clusters)\n",
    "    classification_metrics = format_classification_metrics(train_classification_report, test_classification_report, n_clusters=n_clusters)\n",
    "\n",
    "    return clustering_metrics, classification_metrics\n",
    "\n",
    "def score_spectral(X_train, X_test, y_train, y_test, cluster_counts=None):\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = [2, 3, 5, 10]\n",
    "    clustering_metrics_list = []\n",
    "    classification_metrics_list = []\n",
    "    for n_clusters in cluster_counts:\n",
    "        clustering_scores, classification_scores = evaluate_spectral(X_train, X_test, y_train, y_test, n_clusters)\n",
    "        clustering_metrics_list.append(clustering_scores)\n",
    "        classification_metrics_list.append(classification_scores)\n",
    "\n",
    "    return clustering_metrics_list, classification_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:57:32.004046Z",
     "start_time": "2024-12-03T16:57:30.455335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>Train_DB</th>\n",
       "      <th>Train_Sil</th>\n",
       "      <th>Train_Pur</th>\n",
       "      <th>Test_DB</th>\n",
       "      <th>Test_Sil</th>\n",
       "      <th>Test_Pur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_clusters  Train_DB  Train_Sil  Train_Pur  Test_DB  Test_Sil  Test_Pur\n",
       "0           2      0.93       0.59       0.72     0.83      0.63      0.66\n",
       "1           3      0.92       0.59       0.70     0.82      0.63      0.65\n",
       "2           5      0.80       0.64       0.49     0.74      0.65      0.54\n",
       "3          10      0.95       0.58       0.76     0.92      0.58      0.80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_cluster_metrics, spectral_classification_metrics = score_spectral(X_train, X_test, y_train, y_test)\n",
    "spectral_cluster_results_df = pd.DataFrame(spectral_cluster_metrics)\n",
    "spectral_cluster_reports_df = pd.DataFrame(spectral_classification_metrics)\n",
    "\n",
    "spectral_cluster_results_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spectral_net(X_train, X_test, y_train, y_test, n_clusters=None):\n",
    "    # Define the SpectralNet model\n",
    "    spectralnet = SpectralNet(n_clusters=n_clusters, spectral_hiddens=[128, 64, 16, n_clusters])\n",
    "\n",
    "    # Convert training data to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "    # Train the SpectralNet model on training data\n",
    "    spectralnet.fit(X_train_tensor)\n",
    "\n",
    "    # Predict cluster labels for training data\n",
    "    with torch.no_grad():\n",
    "        train_cluster_labels = spectralnet.predict(X_train_tensor)\n",
    "\n",
    "    # Map clusters to class labels using training data\n",
    "    def derive_cluster_mapping(y_true, cluster_labels):\n",
    "        mapping = {}\n",
    "        for cluster in np.unique(cluster_labels):\n",
    "            labels_in_cluster = y_true[cluster_labels == cluster]\n",
    "            if len(labels_in_cluster) == 0:\n",
    "                continue\n",
    "            mode_result = mode(labels_in_cluster)\n",
    "            most_common = mode_result.mode.item()\n",
    "            mapping[cluster] = most_common\n",
    "        return mapping\n",
    "\n",
    "    cluster_to_class_mapping = derive_cluster_mapping(y_train, train_cluster_labels)\n",
    "\n",
    "    # Align training labels\n",
    "    train_aligned_labels = np.array([cluster_to_class_mapping[label] for label in train_cluster_labels])\n",
    "\n",
    "    # Predict cluster labels for test data\n",
    "    with torch.no_grad():\n",
    "        test_cluster_labels = spectralnet.predict(X_test_tensor)\n",
    "\n",
    "    # Align test labels\n",
    "    test_aligned_labels = np.array([cluster_to_class_mapping.get(label, -1) for label in test_cluster_labels])\n",
    "\n",
    "    # Evaluate performance\n",
    "    train_classification_report = clustering_classification_report(train_aligned_labels, y_train)\n",
    "    test_classification_report = clustering_classification_report(test_aligned_labels, y_test)\n",
    "\n",
    "    train_clustering_scores = score_clustering(X_train, y_train, train_aligned_labels)\n",
    "    test_clustering_scores = score_clustering(X_test, y_test, test_aligned_labels)\n",
    "\n",
    "    clustering_metrics = format_clustering_metrics(train_clustering_scores, test_clustering_scores, n_clusters=n_clusters)\n",
    "    classification_metrics = format_classification_metrics(train_classification_report, test_classification_report, n_clusters=n_clusters)\n",
    "\n",
    "    return clustering_metrics, classification_metrics\n",
    "\n",
    "def score_spectral_net(X_train, X_test, y_train, y_test, cluster_counts=None):\n",
    "    if cluster_counts is None:\n",
    "        cluster_counts = [2, 3, 5, 10]\n",
    "    clustering_metrics_list = []\n",
    "    classification_metrics_list = []\n",
    "    for n_clusters in cluster_counts:\n",
    "        clustering_scores, classification_scores = evaluate_spectral_net(X_train, X_test, y_train, y_test, n_clusters)\n",
    "        clustering_metrics_list.append(clustering_scores)\n",
    "        classification_metrics_list.append(classification_scores)\n",
    "\n",
    "    return clustering_metrics_list, classification_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:57:47.793838Z",
     "start_time": "2024-12-03T16:57:32.201065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SpectralNet:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1212750, Valid Loss: 0.1491229, LR: 0.000100: 100%|██████████| 30/30 [00:08<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SpectralNet:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3893485, Valid Loss: 0.4729806, LR: 0.001000: 100%|██████████| 30/30 [00:08<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SpectralNet:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2403634, Valid Loss: 9.7300339, LR: 0.001000: 100%|██████████| 30/30 [00:09<00:00,  3.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SpectralNet:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.9248409, Valid Loss: 41.8620605, LR: 0.001000: 100%|██████████| 30/30 [00:11<00:00,  2.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>Train_DB</th>\n",
       "      <th>Train_Sil</th>\n",
       "      <th>Train_Pur</th>\n",
       "      <th>Test_DB</th>\n",
       "      <th>Test_Sil</th>\n",
       "      <th>Test_Pur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.80</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_clusters  Train_DB  Train_Sil  Train_Pur  Test_DB  Test_Sil  Test_Pur\n",
       "0           2      0.86       0.62       0.59     0.79      0.64      0.60\n",
       "1           3      0.78       0.65       0.44     0.62      0.68      0.39\n",
       "2           5      0.97       0.57       0.80     4.04      0.17      0.02\n",
       "3          10      1.10       0.53       0.85     4.28      0.14      0.10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectralnet_cluster_metrics, spectralnet_classification_metrics = score_spectral_net(X_train, X_test, y_train, y_test)\n",
    "spectralnet_cluster_results_df = pd.DataFrame(spectralnet_cluster_metrics)\n",
    "spectralnet_cluster_reports_df = pd.DataFrame(spectralnet_classification_metrics)\n",
    "\n",
    "spectralnet_cluster_results_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louvain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louvain_algorithm(X_train, X_test):\n",
    "    modularity_scores = []\n",
    "    k_values = range(5, 30, 5)  # Example range for k\n",
    "\n",
    "    for k in k_values:\n",
    "        # Construct KNN graph\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "        nn.fit(X_train)\n",
    "        distances, indices = nn.kneighbors(X_train)\n",
    "\n",
    "        # Build graph\n",
    "        G = nx.Graph()\n",
    "        for i, neighbors in enumerate(indices):\n",
    "            for j, dist in zip(neighbors, distances[i]):\n",
    "                if i != j:\n",
    "                    G.add_edge(i, j, weight=1 - dist)\n",
    "\n",
    "        # Apply Louvain clustering\n",
    "        partition = community_louvain.best_partition(G)\n",
    "        modularity = community_louvain.modularity(partition, G)\n",
    "        modularity_scores.append((k, modularity))\n",
    "\n",
    "    # Find k with the highest modularity\n",
    "    optimal_k = max(modularity_scores, key=lambda x: x[1])[0]\n",
    "    print(f\"Optimal k based on modularity: {optimal_k}\")\n",
    "\n",
    "    # Map partition dictionary to a list aligned with training data indices\n",
    "    train_clusters = np.array([partition[i] for i in range(len(X_train))])\n",
    "\n",
    "    # Compute cluster prototypes (mean vectors of clusters)\n",
    "    cluster_ids = np.unique(train_clusters)\n",
    "    cluster_prototypes = {}\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "        cluster_members = X_train[train_clusters == cluster_id]\n",
    "        cluster_prototype = cluster_members.mean(axis=0)\n",
    "        cluster_prototypes[cluster_id] = cluster_prototype\n",
    "\n",
    "    def compute_cluster_distances(X, cluster_prototypes):\n",
    "        distances = []\n",
    "        for x in X:\n",
    "            dists = [np.linalg.norm(x - cluster_prototypes[cluster_id]) for cluster_id in cluster_prototypes]\n",
    "            distances.append(dists)\n",
    "        return np.array(distances)\n",
    "\n",
    "    # Compute distances for training data\n",
    "    train_cluster_distances = compute_cluster_distances(X_train, cluster_prototypes)\n",
    "\n",
    "    # Compute distances for test data\n",
    "    test_cluster_distances = compute_cluster_distances(X_test, cluster_prototypes)\n",
    "\n",
    "\n",
    "    # Combine original features with cluster distance features\n",
    "    X_train_with_features = np.hstack((X_train, train_cluster_distances))\n",
    "    X_test_with_features = np.hstack((X_test, test_cluster_distances))\n",
    "\n",
    "    # Convert to DF\n",
    "    X_train_with_features = pd.DataFrame(X_train_with_features)\n",
    "    X_test_with_features = pd.DataFrame(X_test_with_features)\n",
    "\n",
    "    return X_train_with_features, X_test_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k based on modularity: 5\n",
      "Number of features before adding cluster distances: 28\n",
      "Number of features after adding cluster distances: 42\n"
     ]
    }
   ],
   "source": [
    "train_cluster_distances, test_cluster_distances = louvain_algorithm(X_train, X_test)\n",
    "\n",
    "# Print number of features before and after adding cluster distances\n",
    "print(f\"Number of features before adding cluster distances: {X_train.shape[1]}\")\n",
    "print(f\"Number of features after adding cluster distances: {train_cluster_distances.shape[1]}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "   accuracy  precision   recall  f1_score   roc_auc  \\\n",
      "0  0.973948   0.967391  0.89899  0.931937  0.989369   \n",
      "\n",
      "                               classification_report  \\\n",
      "0  {'0.0': {'precision': 0.9754299754299754, 'rec...   \n",
      "\n",
      "                                best_hyperparameters  \n",
      "0  {'solver': 'liblinear', 'penalty': 'l2', 'C': ...  \n"
     ]
    }
   ],
   "source": [
    "results = logistic_regression_classifier(X_train, X_test, y_train, y_test)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comptools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
